<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: python | Sasha Laundy]]></title>
  <link href="http://sursh.github.com/blog/categories/python/atom.xml" rel="self"/>
  <link href="http://sursh.github.com/"/>
  <updated>2014-10-21T10:17:26-04:00</updated>
  <id>http://sursh.github.com/</id>
  <author>
    <name><![CDATA[Sasha Laundy]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Crashing Heroku (or When Memory Management in Python Really Matters)]]></title>
    <link href="http://sursh.github.com/blog/2013/03/07/crashing-heroku-or-when-memory-management-in-python-really-matters/"/>
    <updated>2013-03-07T18:24:00-05:00</updated>
    <id>http://sursh.github.com/blog/2013/03/07/crashing-heroku-or-when-memory-management-in-python-really-matters</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/maxlikely">David</a> and I paired this week to build a <a href="http://www.codinghorror.com/blog/2008/06/markov-and-you.html">Markov chain generator</a>. It analyzes 15MB of old Hacker News headlines and generates completely new headlines that really sound like they came from HN. You can <a href="https://twitter.com/HackerNewsOrNot">follow the Twitter bot</a> or <a href="https://github.com/sursh/markov-hacker-news">check out the code</a>.</p>

<p><a href="http://blog.sashalaundy.com/blog/2013/03/05/help-us-plug-our-memory-leaks/">Last time</a>, we asked your advice on how to handle all possible trigrams in 15 MB of seed text - a task that took up about 1 GB of memory in our first implementation, rapidly blasting through Heroku's limit of 512 MB. The culprit? <strong>Dictionaries.</strong></p>

<p>Our initial data structure was a dict of dicts. The key was a <em>bigram</em>, or word pair, and the value was a dict of all of the words that followed that bigram in the training text and the number of times they occured. Wat? Here's an example:</p>

<p>``` python Our initial data structure</p>

<p>{
  ('how', 'to'): { 'do': 529,</p>

<pre><code>               'grow': 202,
               'know': 134,
               .....
             },
</code></pre>

<p>  ('to', 'do'): { 'the': 1245,</p>

<pre><code>              'everything': 149,
              .....
            },
</code></pre>

<p>  ...
}</p>

<p>```</p>

<p>With 15 MB of training text and so much redundancy, this can get out of hand quickly! Every word appears <em>at least</em> 3 times in the matrix, and some appear thousands of times.</p>

<h3>Possible solutions:</h3>

<ul>
<li><strong>Generators.</strong> for iterating over the headlines. This helped a lot - got us from 1.3 G down to about 850 MB.</li>
<li><strong>Compression.</strong> Tossing out common values, like <code>1</code> for the count of the long tail of words.</li>
<li><strong>Creative use of data types.</strong> E.G. storing words as <code>int</code>s instead of <code>string</code>s.</li>
<li><strong>Lists.</strong> They take up less memory than dicts. One variation that we tried (and is in <a href="https://github.com/sursh/markov-hacker-news/blob/e584ef95ade08f03a482debfc0c0f47adde67af3/trigrams.py">this version of our code</a>) is creating two dicts of lists rather than one dict of dicts.</li>
<li><strong>Trade memory for speed.</strong> Darius, a Hacker School alum, pointed us to <a href="http://stackoverflow.com/questions/327223/memory-efficient-alternatives-to-python-dictionaries/327295#327295">his Stack Overflow answer</a>, where he laid out a few other approaches that mainly trade memory usage for speed. Also, David dug up some recent papers that focus entirely on this problem. Researchers are currently doing some very clever algorithms to get the most out of their data structures.</li>
</ul>


<h3>So where did we leave it?</h3>

<p>Darius' suggestions and the approaches in the literature will certainly help reduce memory usage. We wanted to maximize learning per unit of time at Hacker School and didn't think implementing these approaches teach us proportionally more than reading about them. We shipped it by deploying on a machine with more memory. Follow the Twitter bot <a href="https://twitter.com/HackerNewsOrNot">here</a>!</p>

<h4>Stuff I learned along the way:</h4>

<p> Generators, classes and methods in python, pickle, regex, defaultdict, Tweepy, Buffer's API, how bad the built in titlecase methods are in Python, some clever way to handle default arguments, some fundamental principles of NLP.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Help us plug our memory leaks]]></title>
    <link href="http://sursh.github.com/blog/2013/03/05/help-us-plug-our-memory-leaks/"/>
    <updated>2013-03-05T00:08:00-05:00</updated>
    <id>http://sursh.github.com/blog/2013/03/05/help-us-plug-our-memory-leaks</id>
    <content type="html"><![CDATA[<p>David and I are running our app on Heroku, but it uses too much memory to run! <strong>Got any clever optimization suggestions?</strong></p>

<p>We're building a Markov chain generator that is trained on a corpus of 15MB of old Hacker News headlines. The app currently indexes the training corpus and constructs a matrix of all the possibilities, then queries that matrix to generate a hopefully entertaining new headline. Preview it <a href="http://twitter.com/HackerNewsOrNot">here</a>.</p>

<p><strong>However, as we first wrote it, it used 1+ GB of memory when we deployed to Heroku, where the limit is 512 MB!</strong> We wrangled it down to 570 MB today and will tackle it again tomorrow morning, but need more ideas.</p>

<p>Some things we've tried:</p>

<ul>
<li>Pickling the matrix. Not pickling the matrix. (didn't help)</li>
<li>Storing the words as <code>int</code>s in our matrix rather than <code>string</code>s (helped)</li>
<li>Tweaked the data structure (helped, could probably do more with this)</li>
<li>We ran out of time, but want to try some sort of data store. Redis?</li>
</ul>


<p><strong>What should we try next?</strong> Our code after the jump:</p>

<!-- more -->


<p>Very unfinished code:</p>

<p>``` python The code + a few half-finished experiments
import sys
import time
import re
import collections
import numpy
import string
import cPickle as pickle
import twitterclient</p>

<p>class Markov(object):</p>

<p>  def read(self, filename):</p>

<pre><code>with open(filename) as f:
  for line in f:
    headline = map(intern, re.split('\s+', line.lower().strip()))
if headline[-1] == '': del(headline[-1]) # clear lingering null entries
if headline[0] == '': del(headline[0])
    yield ['^'] + headline + ['$']
</code></pre>

<p>  def generateTrigrams(self, tokens):</p>

<pre><code>''' Create a list of tuples, where each tuple is a trigram '''
for idx, item in enumerate(tokens[:-2]):
  yield (item, tokens[idx+1], tokens[idx+2])
</code></pre>

<p>  def generateMatrix(self, filename):</p>

<pre><code>''' Run through the list of trigrams and add them to the occurence matrix '''

self.matrix = {}
self.dictionary = {}

try: # load as pickle

  with open('matrix.p', 'rb') as f:
    self.matrix = pickle.load(f)

except: # interpret file as newline separated headlines

  print 'reading headlines'
  headlines = self.read(filename)

  for headline in headlines:

    trigrams = self.generateTrigrams(headline)

    for first_word, second_word, third_word in trigrams:
      self.dictionary.setdefault(first_word, len(self.dictionary))
      self.dictionary.setdefault(second_word, len(self.dictionary))
      self.dictionary.setdefault(third_word, len(self.dictionary))

    trigrams = self.generateTrigrams(headline)

    for first_word, second_word, third_word in trigrams:
      first_word  = self.dictionary[first_word]
      second_word = self.dictionary[second_word]
      third_word  = self.dictionary[third_word]

      self.matrix.setdefault( (first_word, second_word), collections.defaultdict(int)) 
      self.matrix[(first_word, second_word)][third_word] += 1

  print sum(len(d) for d in self.matrix.itervalues())
  #print("DUMPING %d" % len(self.matrix))
  #pickle.dump(self.matrix, open("matrix.p", "wb"))
</code></pre>

<p>  def generateNextWord(self, prev_word, current_word):</p>

<pre><code>conditional_words = self.matrix[ (prev_word, current_word) ]
words, counts = zip(*conditional_words.items())

# pick one of the possibilities, with probability weighted by frequency in training corpus
cumcounts = numpy.cumsum(counts)
coin = numpy.random.randint(cumcounts[-1])
for index, item in enumerate(cumcounts):
  if item &gt; coin:
    return words[index]
</code></pre>

<p>  def generateParagraph(self, seed2 = 'i', seed1 = '<sup>',):</sup></p>

<pre><code>seed1 = self.dictionary[seed1]
seed2 = self.dictionary[seed2]

prev_word = seed1
current_word = seed2
paragraph = [ seed1 ]

while (self.dictionary[current_word] != '$' and len(paragraph) &lt; 20): 
  paragraph.append(current_word)
  prev_word, current_word = current_word, self.generateNextWord( prev_word, current_word )

#paragraph = []
paragraph = ' '.join(paragraph[1:])  # strip off leading caret
return string.capwords(paragraph)
</code></pre>

<p>  def <strong>str</strong>(self):</p>

<pre><code>s = ''
for word, cfd in self.matrix.iteritems():
  for word2, count in cfd.iteritems():
    s += '%s %s: %d\n' % (word, word2, count)
return s
</code></pre>

<p>def main():</p>

<p>  if len(sys.argv) != 2:</p>

<pre><code>print 'usage: ./markov.py inputFile'
sys.exit(1)
</code></pre>

<p>  filename = sys.argv[1]</p>

<p>  m = Markov()
  m.generateMatrix(filename)</p>

<p>  while True:</p>

<pre><code>tweet = m.generateParagraph('how') # todo: new seed
break
if len(tweet) &lt; 120:
  break
</code></pre>

<p>  print("Tweeting %s" % tweet)
  twitterclient.postTweet(tweet)</p>

<p>if <strong>name</strong> == '<strong>main</strong>':
  main()</p>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[In Search of Funny Gibberish]]></title>
    <link href="http://sursh.github.com/blog/2013/03/04/in-search-of-funny-gibberish/"/>
    <updated>2013-03-04T17:31:00-05:00</updated>
    <id>http://sursh.github.com/blog/2013/03/04/in-search-of-funny-gibberish</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/maxlikely">David</a> and I paired this week to build a bot that analyzes 15MB of old Hacker News and generates completely new headlines that really sound like they came from HN. You can <a href="https://twitter.com/HackerNewsOrNot">follow the Twitter bot</a> or <a href="https://github.com/sursh/markov-hacker-news">check out our code</a>.</p>

<h3>Inspiration</h3>

<p>I was inspired by <a href="http://www.linkedlistnyc.org/archive/issue_081.html">this Hacker News parody</a> and <a href="https://twitter.com/joedamarkov">this Twitter bot</a> to build a Markov chain generator as my warm-up project in Python.</p>

<h3>A lightweight introduction to Markov chain generators</h3>

<p>Markov chain generators are an interesting and dead simple way to generate a chain of anything - words, weather predictions, market simulations - anything that has happened before and can happen again.</p>

<p>The chain is generated by examining the current link in the chain, picking the next link based on what typically followed the first link in the training corpus. Then, it <em>completely forgets</em> about the initial link. It chooses the third link <em>only</em> based on what generally comes after the second link. And so forth.</p>

<p>So there is no history, no memory. The generator can switch between training sentences stochastically, so it can generate sentences that sound kind of like the original source, but don't necessarily make any sense, and are hopefully funny.</p>

<h3>Bigrams vs trigrams</h3>

<p>The first design decision to make is how much history to examine when choosing a new link in your chain. If you only go one word back, you are looking at pairs of words, or <em>bigrams</em>.</p>

<p>Some sentences generated with bigrams: (please pardon that they're all seeded with 'why' - this is from an early branch)</p>

<p><code>
Why I Am I Talk About To Be Designing For The Daily Check Out Of Braid Creating False
Why Machines Is Right Fit
Why Nokia Partners
Why We Doing In Siberia
Why Are Literally Amp Chrome Opera Singer Is Coming Soon
Why The Twitter Besides Buying Groupon Will Not Good Freely Available To Effectively Off
Why Objective C Safer In Gears
Why Some Sleep Deprived Brains
Why I Like Instapaper Redesigns Foursquare Checkin Offers Readers Cause Problems And Should Set Theorists
Why Dropbox S More Music Gear Online Teaching Ror Developers
</code></p>

<p>They are nice and random, and clearly contain the right buzzwords, but they aren't very grammatical and therefore can't be funny. <strong>Humor relies on surprise:</strong> setting up an expectation, then delivering something different. With bigrams, you never get a coherent enough sentence to generate an expectation, so no shot at being funny.</p>

<p>Let's smooth out our chains by moving from bigrams to trigrams. So instead of looking at what follows a given individual word in our training corpus, we'll look at what follows a pair of words. Here are some examples - note that the grammar is significantly better and some are worth a chuckle. I particularly like 7 and 10.</p>

<p><code>
My Year Of Experience Is A Big Twist
Engine Yard
Mini-microsoft: Compensatory Arrangements Of Certain (microsoft) Officers
The 12-step Landing Page
More Webmaster Questions - Answered
Scripting Gnu Screen
The Full Social Network Buttons To Operate On Your Terrorist Neighbor
Typical Tech Entrepreneur?
The Lost Lesson Of 'free'
Contact Lenses Are Curing The Founder's Syndrome
</code></p>

<h3>Which seed?</h3>

<p>HN is at its least self conscious and most easily lampooned when doling out advice. Consider these 'how'- and 'why'-seeded sentences:</p>

<p><code>
How To Choose The Right People And The Chance To Present
Why Javascript Is Broken
How Do You Manage Your Startupâ€™s Pr At Tech Startups Are Moving To Rackspace
Why Do Organic Eggs Come In Bunches
How Apple Is The Prevalence Of Qwerty
Why Google Wants To Magically Transfer Gov Debt To Darwin
How To Finance Your App From The Lhc Will See Global History Of Governments And Geeks Parse The World
Why Are Bank Security Questions On Agile
How To Hack The Us - So Stock Up 879.55%
Why Computer Displays Suck For You
</code></p>

<p>When we choose a first word randomly from all the words that have ever been in headlines, we get a bigger assortment, but I think they're less funny:</p>

<p><code>
Canonical Contributes Only 1% Of Profit
Google Uneveils New Search Results With Google's Closure Of Paid Prioritization
What Do You Deal With Worldnow, Adds 19 Million Potential Users
Diminishing Dead-tree Media And Mobile Computing Is A Beautiful Monster
Ask Hn Yahoos: What Yahoo Should Do To Excel
China Demands New Pcs Is Ruined By A Thousand Years
Freemium: A Business Plan Competition
Buy My Blog, Please
Why I Am In Your Field?
8 Tips To Considerate When Planning To Move Themselves (neural Network)
</code></p>

<p>The other drawback is that they're more likely to hit on a seed with only one possible resulting sentence, like "Buy My Blog, Please," above.</p>

<p><code>bash
$ grep -i "buy my blog" ../hnfull.txt
Gawker media boss Nick Denton: Buy my blog, please
</code></p>

<p>If you think of the bot as walking through the possibilities, a common seed like 'how' will branch of lots of different ways, so there are many paths for the bot to walk and thus many possible sentence outcomes. A less common seed, like "scripting" will result in fewer possible paths, and more likely to just return a real headline verbatim. There were only 10 ways to finish "buy my" in our corpus of 350,000 training headlines, compared to 7,710 ways to finish "how to."</p>

<h3>Raising the stakes</h3>

<p>But how to make it as funny as possible? Some possible improvements:</p>

<ul>
<li><strong>Crowdsource funniness ratings</strong> (with mechanical turk or a 'hot or not' app, etc). Only tweet out the funniest headlines.</li>
<li>Feed the funniness ratings back into the algorithm. For example, only use the <strong>funniest seeds.</strong></li>
<li>Do <strong>semantic analysis</strong> of parts of speech in the training corpus and use them with templates. This would improve grammar but decrease spontaneity.</li>
<li>Hire Nick and Dave to crank out <a href="http://www.linkedlistnyc.org/archive/issue_081.html">more of these</a> :)</li>
</ul>


<p>Any other ideas?</p>
]]></content>
  </entry>
  
</feed>
